{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This tutorial demonstrates how to use the `flow-judge` library to perform language model-based evaluations using Flow-Judge-v0.1 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an evaluation\n",
    "\n",
    "Running an evaluation is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 13:37:45 awq_marlin.py:89] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "WARNING 09-17 13:37:45 config.py:378] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 09-17 13:37:45 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='flowaicom/Flow-Judge-v0.1-AWQ', speculative_config=None, tokenizer='flowaicom/Flow-Judge-v0.1-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=flowaicom/Flow-Judge-v0.1-AWQ, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n",
      "INFO 09-17 13:37:46 model_runner.py:915] Starting to load model flowaicom/Flow-Judge-v0.1-AWQ...\n",
      "INFO 09-17 13:37:46 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 13:37:47 weight_utils.py:280] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554aca8353df4b81b78626f33c15959a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 13:37:47 model_runner.py:926] Loading model weights took 2.1861 GB\n",
      "INFO 09-17 13:37:49 gpu_executor.py:122] # GPU blocks: 3084, # CPU blocks: 682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.29s/it, est. speed input: 341.32 toks/s, output: 46.56 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from flow_judge.models.model_factory import ModelFactory\n",
    "from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Create a model using ModelFactory\n",
    "model = ModelFactory.create_model(\"Flow-Judge-v0.1-AWQ\") # ! Replace with \"Flow-Judge-v0.1_HF_no_flsh_attn\" is running on no Ampere GPUs\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_judge = FlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Sample to evaluate\n",
    "user_instructions = \"\"\"Please read the technical issue that the user is facing and help me create a detailed solution based on the context provided.\"\"\"\n",
    "customer_issue = \"\"\"I'm having trouble when uploading a git lfs tracked file to my repo: (base)  bernardo@bernardo-desktop  ~/repos/lm-evaluation-harness  ↱ Flow-Judge-v0.1_evals  git push                                            \n",
    "batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\"\"\"\n",
    "context = \"\"\"Configuring Git Large File Storage\n",
    "Once Git LFS is installed, you need to associate it with a large file in your repository.\n",
    "\n",
    "Platform navigation\n",
    "Mac\n",
    "Windows\n",
    "Linux\n",
    "If there are existing files in your repository that you'd like to use GitHub with, you need to first remove them from the repository and then add them to Git LFS locally. For more information, see \"Moving a file in your repository to Git Large File Storage.\"\n",
    "\n",
    "If there are referenced Git LFS files that did not upload successfully, you will receive an error message. For more information, see \"Resolving Git Large File Storage upload failures.\"\n",
    "\n",
    "Open Terminal.\n",
    "\n",
    "Change your current working directory to an existing repository you'd like to use with Git LFS.\n",
    "\n",
    "To associate a file type in your repository with Git LFS, enter git lfs track followed by the name of the file extension you want to automatically upload to Git LFS.\n",
    "\n",
    "For example, to associate a .psd file, enter the following command:\n",
    "\n",
    "$ git lfs track \"*.psd\"\n",
    "> Tracking \"*.psd\"\n",
    "Every file type you want to associate with Git LFS will need to be added with git lfs track. This command amends your repository's .gitattributes file and associates large files with Git LFS.\n",
    "\n",
    "Note: We strongly suggest that you commit your local .gitattributes file into your repository.\n",
    "\n",
    "Relying on a global .gitattributes file associated with Git LFS may cause conflicts when contributing to other Git projects.\n",
    "Including the .gitattributes file in the repository allows people creating forks or fresh clones to more easily collaborate using Git LFS.\n",
    "Including the .gitattributes file in the repository allows Git LFS objects to optionally be included in ZIP file and tarball archives.\n",
    "Add a file to the repository matching the extension you've associated:\n",
    "\n",
    "git add path/to/file.psd\n",
    "Commit the file and push it to GitHub:\n",
    "\n",
    "git commit -m \"add file.psd\"\n",
    "git push\n",
    "You should see some diagnostic information about your file upload:\n",
    "\n",
    "> Sending file.psd\n",
    "> 44.74 MB / 81.04 MB  55.21 % 14s\n",
    "> 64.74 MB / 81.04 MB  79.21 % 3s\"\"\"\n",
    "response = \"\"\"# Resolving Git LFS Data Quota Issue\n",
    "\n",
    "\n",
    "\n",
    "## Problem\n",
    "The user is encountering an error when trying to push Git LFS tracked files:\n",
    "```\n",
    "This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
    "```\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. **Understand the issue**\n",
    "   - The error message indicates that the repository has exceeded its data quota for Git LFS (Large File Storage).\n",
    "   - This is likely due to the size or number of large files being tracked by Git LFS.\n",
    "\n",
    "2. **Check current LFS usage**\n",
    "   - Run the following command to see the current LFS objects:\n",
    "     ```\n",
    "     git lfs ls-files\n",
    "     ```\n",
    "   - This will show you which files are being tracked by Git LFS and their sizes.\n",
    "\n",
    "3. **Review Git LFS pricing and quotas**\n",
    "   - Visit GitHub's pricing page to understand the current LFS storage and bandwidth limits for your account type.\n",
    "   - Free accounts typically have lower limits compared to paid plans.\n",
    "\n",
    "4. **Options to resolve the issue**\n",
    "\n",
    "   a. **Purchase more data packs**\n",
    "      - If you need to keep all current LFS objects and continue using LFS extensively:\n",
    "        1. Go to your GitHub account settings.\n",
    "        2. Navigate to the Billing section.\n",
    "        3. Look for options to purchase additional data packs for Git LFS.\n",
    "\n",
    "   b. **Optimize repository**\n",
    "      - If purchasing more data is not an option, consider optimizing your repository:\n",
    "        1. Remove unnecessary large files from LFS tracking.\n",
    "        2. Use `.gitignore` to exclude large files that don't need version control.\n",
    "        3. Consider using alternative storage solutions for very large assets.\n",
    "\n",
    "   c. **Clean up LFS cache**\n",
    "      - Sometimes, cleaning up the LFS cache can help:\n",
    "        ```\n",
    "        git lfs prune\n",
    "        ```\n",
    "\n",
    "5. **Reconfigure Git LFS tracking**\n",
    "   - Review your `.gitattributes` file to ensure only necessary file types are tracked:\n",
    "     ```\n",
    "     git lfs track\n",
    "     ```\n",
    "   - Modify tracking as needed:\n",
    "     ```\n",
    "     git lfs track \"*.psd\"  # Track PSD files\n",
    "     git lfs untrack \"*.zip\"  # Stop tracking ZIP files\n",
    "     ```\n",
    "\n",
    "6. **Commit changes and retry push**\n",
    "   - After making necessary changes:\n",
    "     ```\n",
    "     git add .gitattributes\n",
    "     git commit -m \"Update Git LFS tracking\"\n",
    "     git push\n",
    "     ```\n",
    "\n",
    "7. **If issues persist**\n",
    "   - Contact GitHub support for further assistance.\n",
    "   - They may be able to provide more specific guidance based on your account and repository details.\n",
    "\n",
    "Remember to regularly monitor your Git LFS usage to avoid hitting quotas in the future. Consider setting up alerts or regularly checking your GitHub account's storage usage statistics.\"\"\"\n",
    "\n",
    "# Create an EvalInput\n",
    "# We want to evaluate the response to the customer issue based on the context and the user instructions\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"user_instructions\": user_instructions},\n",
    "        {\"customer_issue\": customer_issue},\n",
    "        {\"context\": context}\n",
    "    ],\n",
    "    output=response,\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = faithfulness_judge.evaluate(eval_input, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is mostly consistent with the context given. It addresses the specific issue of a Git LFS data quota being exceeded and offers a detailed solution. The response includes steps such as checking current LFS usage, reviewing Git LFS pricing and quotas, and optimizing the repository. It also suggests purchasing more data packs, which aligns with the context's suggestion to purchase more data packs to restore access.\n",
       "\n",
       "However, there are a few minor inconsistencies and additions that are not explicitly mentioned in the context:\n",
       "\n",
       "1. The response suggests running `git lfs ls-files` to check current LFS usage, which is a good practice but not mentioned in the provided context.\n",
       "2. It includes a suggestion to clean up the LFS cache using `git lfs prune`, which is a helpful tip but not part of the given context.\n",
       "3. The response advises setting up alerts or regularly checking GitHub account's storage usage statistics, which is a proactive measure but not mentioned in the context.\n",
       "\n",
       "Despite these minor additions, the vast majority of the content is supported by the context, and the response does not introduce any significant hallucinated or fabricated information. Therefore, the response is mostly consistent with the provided context.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the result\n",
    "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running batched evaluations\n",
    "\n",
    "The `FlowJudge` class also supports batch evaluation. This is useful when you want to evaluate multiple samples at once in Evaluation-Driven Development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.24s/it, est. speed input: 1040.28 toks/s, output: 190.25 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the sample data\n",
    "import json\n",
    "with open(\"sample_data/csr_assistant.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list of inputs and outputs\n",
    "inputs_batch = [\n",
    "    [\n",
    "        {\"user_instructions\": sample[\"user_instructions\"]},\n",
    "        {\"customer_issue\": sample[\"customer_issue\"]},\n",
    "        {\"context\": sample[\"context\"]}\n",
    "    ]\n",
    "    for sample in data\n",
    "]\n",
    "outputs_batch = [sample[\"response\"] for sample in data]\n",
    "\n",
    "# Create a list of EvalInput\n",
    "eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "                         \n",
    "# Run the batch evaluation\n",
    "results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Sample 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response is mostly consistent with the provided context, but it introduces some minor inconsistencies and fabrications. \n",
       "\n",
       "1. The suggestion to use Git LFS is appropriate and consistent with the context.\n",
       "2. The step to install Git LFS is also correct and aligns with the context.\n",
       "3. The instruction to set up Git LFS for the user account is accurate.\n",
       "4. The suggestion to track large files using `git lfs track` is correct, but it assumes the user knows the file extensions, which is not explicitly mentioned in the context.\n",
       "5. The instruction to add a .gitattributes file is not mentioned in the context, which only briefly mentions that Git LFS replaces large files with text pointers inside Git.\n",
       "6. The commands to add and commit large files, and to push changes, are generally correct but lack the specificity of using Git LFS for handling large files.\n",
       "\n",
       "Overall, the response is mostly consistent with the context, but it introduces some minor inconsistencies and fabrications that are not fully supported by the provided context.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is mostly consistent with the context but contains several significant inaccuracies and misleading instructions that deviate from the context. \n",
       "\n",
       "1. The first step correctly instructs to check existing remotes using `git remote -v`, which is consistent with the context.\n",
       "2. The second step incorrectly suggests using `git remote set-url origin new-url` to keep the URL unchanged, which contradicts the context that implies changing the URL.\n",
       "3. The third step incorrectly suggests using `git remote add new-remote-name new-url` to remove a remote, which is the opposite of what the context suggests.\n",
       "4. The fourth step incorrectly combines commands to remove and add a remote, which is not supported by the context.\n",
       "\n",
       "Additionally, the response includes misleading information such as suggesting that these changes will definitely result in the same error, which is not supported by the context. The response also introduces fabricated elements like \"Replace 'new-url' with the exact same URL you're currently using\" and \"Replace 'new-remote-name' with the name of an existing remote, and 'new-url' with any random string,\" which are not present in the context.\n",
       "\n",
       "Overall, while the response does contain some correct information from the context, it is mostly inconsistent and contains significant amounts of fabricated information that deviate from the provided context.\n",
       "\n",
       "__Score:__\n",
       "2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is mostly consistent with the context given. It accurately describes the `git revert` command and provides a step-by-step guide on how to use it, which aligns well with the context. The response also mentions the importance of having a backup, which is a good practice as suggested in the context.\n",
       "\n",
       "However, there are a few minor inconsistencies and additions that are not explicitly mentioned in the context:\n",
       "\n",
       "1. The response suggests using `git log` to find the commit hash, which is a reasonable suggestion but not mentioned in the context.\n",
       "\n",
       "2. The response includes a command for reverting multiple commits using a range, which is a useful addition but not explicitly stated in the context.\n",
       "\n",
       "3. The suggestion to create a backup branch before performing any Git operations is good advice but not directly mentioned in the context.\n",
       "\n",
       "These minor additions and suggestions, while helpful, are not fabricated or hallucinated information but rather practical tips that go beyond the given context. Therefore, the response is mostly consistent with the context but includes some additional helpful information.\n",
       "\n",
       "Overall, the response is very close to being completely consistent with the provided context, with only minor and inconsequential inconsistencies or fabrications.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 4:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is completely inconsistent with the given context. The context outlines a series of specific steps to remove sensitive data from a Git repository, including using tools like BFG Repo-Cleaner or git filter-branch, force-pushing changes to GitHub, contacting GitHub Support, advising collaborators to rebase, and using git commands to remove old references. However, the response suggests that no action is needed and that Git will automatically handle the removal of sensitive data, which directly contradicts the context provided. Therefore, the response contains significant hallucinated information that is not supported by the context.\n",
       "\n",
       "Based on the evaluation criteria and scoring rubric, the response fits the description for a score of 1, as it is completely inconsistent with the provided context and contains hallucinated information.\n",
       "\n",
       "__Score:__\n",
       "1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 5:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response is mostly consistent with the provided context, with only minor and inconsequential inconsistencies. The response accurately describes the steps to resolve merge conflicts, including opening the conflicted file, identifying conflict markers, deciding which changes to keep, editing the file, saving, staging, and committing the changes. It also mentions using `git mergetool` as an alternative method, which aligns with the context.\n",
       "\n",
       "However, there are a few minor issues:\n",
       "1. The response suggests using `git add <filename>` instead of `git add <filename>`, which is a minor typo but does not significantly impact the overall consistency.\n",
       "2. The response includes a tip about minimizing merge conflicts in the future, which, while helpful, is not part of the original context.\n",
       "\n",
       "Overall, the response is faithful to the context and does not contain significant hallucinated or fabricated information. The minor inconsistencies and additional tip do not detract from the overall accuracy and relevance of the response.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 6:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response is highly consistent with the provided context. It accurately describes the process of adding a remote repository and pushing local changes to a remote repository using Git. The steps outlined in the response directly align with the context given, including the use of the 'git remote add' command and the 'git push -u origin main' command. There are no hallucinated or fabricated details in the response; all information is supported by the context provided. The response is clear, concise, and directly applicable to the customer's issue, making it a faithful and accurate representation of the context.\n",
       "\n",
       "__Score:__\n",
       "5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the results\n",
    "for i, result in enumerate(results):\n",
    "    display(Markdown(f\"__Sample {i+1}:__\"))\n",
    "    display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "\n",
    "When running batched evaluation, it's usually recommended to save the results to a file for future reference and reproducibility. This is the default behavior of the evaluate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.28s/it, est. speed input: 1007.69 toks/s, output: 199.25 toks/s]\n",
      "INFO:flow_judge.flow_judge:Saving results to output/\n"
     ]
    }
   ],
   "source": [
    "# Run the batch evaluation\n",
    "results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of output:\n",
      "[PosixPath('output/response_faithfulness_5-point_likert')]\n",
      "\n",
      "Contents of output/response_faithfulness_5-point_likert:\n",
      "[PosixPath('output/response_faithfulness_5-point_likert/metadata_response_faithfulness_5-point_likert_flowaicom__Flow-Judge-v0.1-AWQ_vllm_2024-09-17T07-24-25.410.jsonl'), PosixPath('output/response_faithfulness_5-point_likert/results_response_faithfulness_5-point_likert_flowaicom__Flow-Judge-v0.1-AWQ_vllm_2024-09-17T11-41-16.298.jsonl'), PosixPath('output/response_faithfulness_5-point_likert/metadata_response_faithfulness_5-point_likert_flowaicom__Flow-Judge-v0.1-AWQ_vllm_2024-09-17T11-41-16.298.jsonl'), PosixPath('output/response_faithfulness_5-point_likert/results_response_faithfulness_5-point_likert_flowaicom__Flow-Judge-v0.1-AWQ_vllm_2024-09-17T07-24-25.410.jsonl')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "latest_run = next(output_dir.iterdir())\n",
    "\n",
    "print(f\"Contents of {output_dir}:\")\n",
    "print(list(output_dir.iterdir()))\n",
    "\n",
    "print(f\"\\nContents of {latest_run}:\")\n",
    "print(list(latest_run.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each evaluation run generates 2 files:\n",
    "- `results_....json`: Contains the evaluation results.\n",
    "- `metadata_....json`: Contains metadata about the evaluation for reproducibility.\n",
    "\n",
    "These files are saved in the `output` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
